{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Acquire the dataset\n",
    "\n",
    "We got the data from this google drive link. https://drive.google.com/drive/folders/16BK8_d1V-A3M1WQ0neaeCwqrHPfzH7QS?usp=sharing . This link is provided from Wei Shen where he gotten it from Mr.Afiq, who is the founder of Chili Lado.\n",
    "\n",
    "We decided not to use all the datasets in this google drive, however, we only selected the datasets that are relevant to our analysis. The selected the Product Overview datasets from the Product Folder. \n",
    "\n",
    "At first we downloaded all the dataset into our local drive by using the dowload all button.\n",
    "\n",
    "![DownloadAll](ChiliLadoData/DownloadAll.png)\n",
    "\n",
    "All the datasets is downloaded in this zipped file.\n",
    "\n",
    "![ZippedFile](ChiliLadoData/ZippedFile.png)\n",
    "![DownloadedFile](ChiliLadoData/DownloadedFile.png)\n",
    "\n",
    "Based on our observation, the zip include the Product Overview dataset which is from May 2023 to September 2023. All of them has 22 Column, however it has different number of rows. May, June, July, August, September has 32,31,32,32,31 rows respectively. The column names are:\n",
    "1. Date\n",
    "2. Product Visitors (Visit)\n",
    "3. Product Page Views\n",
    "4. Items Visited\n",
    "5. Product Bounce Visitors\n",
    "6. Product Bounce Rate\n",
    "7. Search Clicks\n",
    "8. Likes\n",
    "9. Product Visitors (Add to Cart)\n",
    "10. Units (Add to Cart)\n",
    "11. Conversion Rate (Add to Cart)\n",
    "12. Buyers (Placed Order)\n",
    "13. Units (Placed Order)\n",
    "14. Items Placed\n",
    "15. Sales (Placed Order)(MYR)\n",
    "16. Conversion Rate (Placed Order)\n",
    "17. Buyers (Confirmed Order)\n",
    "18. Units (Confirmed Order)\n",
    "19. Items Confirmed\n",
    "20. Sales (Confiremd Order)(MYR)\n",
    "21. Conversion Rate (Confirmed Order)\n",
    "22. Converison Rate (Placed to Confirmed)\n",
    "\n",
    "We first combine all the 5 files together, however we decided to do it with copy and paste instead of using python code because it only has 5 files. We use Ctrl+C to copy all the rows and use Ctrl+V to paste the copied rows into a new Excel File called MergedFile.xlsx.\n",
    "\n",
    "![CopiedFile](ChiliLadoData/CopiedFile.png)\n",
    "\n",
    "![PasteFile](ChiliLadoData/PasteFile.png)\n",
    "\n",
    "We copied all five datasets into the MergedFile. However, for May 2023, we copy the whole file including the column names, while for other months,we only copied the data. We pasted the data beneath May 2023. We followed the same process for July 2023 and subsequent months.\n",
    "\n",
    "![MayJune](ChiliLadoData/MayJune.png)\n",
    "\n",
    "To check if the data is merged correctly, we calculate the total number of rows by adding the number of days in these 5 months and the row contains attribute name which is 31 + 30 + 31 + 31 + + 30 + 1 = 154, as our MergedFile has 154 rows means that we had merged it correctly.\n",
    "\n",
    "To fulfill our objective, we require a different set of data sourced from the Dashboard of the year 2023.\n",
    "\n",
    "![Dashboard2023](ChiliLadoData/Dashboard2023.png)\n",
    "\n",
    "We get these following columns:\n",
    "\n",
    "1. Numbers of buyers\n",
    "2. Numbers of new buyers\n",
    "3. Numbers of existing buyers\n",
    "\n",
    "We replicate the procedure by copying the column and its data as the method above. Then, we paste this data into our recently created merged file.\n",
    "\n",
    "![AddedColumn](ChiliLadoData/AddedColumn.png)\n",
    "\n",
    "We found out that the figures in repeat purchase rate numbers are inaccurate. So, We perform data augmentation for two columns: the percentage of new buyers and the percentage of repeat buyers by using the data inside the dataset and Excel Function.\n",
    "\n",
    "![NewCalculation](ChiliLadoData/NewCalculation.png)\n",
    "\n",
    "![RepeatCalculation](ChiliLadoData/RepeatCalculation.png)\n",
    "\n",
    "We use If in our calculation because if the number of buyer is equal to zero, it might have division by zero error. If the number of buyers equals zero, the output is set to zero, otherwise, the division operation proceeds.  We also convert our calculation to percentage by using this function.\n",
    "\n",
    "![Percentage](ChiliLadoData/Percentage.png)\n",
    "\n",
    "We decide to use all these steps in Excel rather in Python because it is faster than code, also we want to make direct changes to our dataset rather than temporary changes only.\n",
    "\n",
    "Now the data can be used for the next few steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Import the dataset\n",
    "\n",
    "We import the dataset from our local directory. We created a file named ChiliLadoData and stored all the dataset and picture used there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('MergedFile.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Clean the data by dentifying and handling missing value, redundancy and outliers\n",
    "\n",
    "Initially, almost all the data in the Excel file was not numerical data.\n",
    "\n",
    "![ConvertData](ChiliLadoData/ConvertData.png)\n",
    "\n",
    "Therefore, we converted all the data in the dataset into numerical values by selecting the \"Convert to Number\" option in Excel to prevent potential errors. You can identify non-numeric data when the left upper corner of the cell is marked in green.\n",
    "\n",
    "![Number](ChiliLadoData/Number.png)\n",
    "\n",
    "If all the cells are white, it indicates that we have successfully converted the data into numerical values. Now, we can proceed with using Python for data preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the missing value of each column by using .isna(), use .sum() to sum all the missing value\n",
    "print(\"Find missing value of each column using isna()\")\n",
    "print (df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We found out that there are no row with missing value so we do not need to delete or drop any row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine any redundancy in the dataset\n",
    "# Use .duplicate is to check if there is any duplicate data\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "duplicate_columns = df.T.duplicated().sum()\n",
    "\n",
    "print(\"Find any duplicate values:\")\n",
    "duplicate_rows, duplicate_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this, we found out that there are no duplicate data in this dataset so there are no redundancy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check for outliers in the data\n",
    "\n",
    "# Exclude the 'Date' column\n",
    "outliersdf = df.drop(columns=['Date'])\n",
    "\n",
    "# Create a boxplot to visualize the outliers\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=outliersdf)\n",
    "plt.title(\"Boxplot of Data\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the boxplot data graph before cleaning the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the first quartile (Q1), third quartile (Q3) and interquantile range (IQR)\n",
    "Q1 = outliersdf.quantile(0.25, numeric_only=True)\n",
    "Q3 = outliersdf.quantile(0.75, numeric_only=True)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Identify and filter the outliers using the IQR method\n",
    "outlier_condition = ((outliersdf < (Q1 - 1.5 * IQR)) | (outliersdf > (Q3 + 1.5 * IQR))).any(axis=1)\n",
    "\n",
    "# Dataframe with outliers are removed\n",
    "df = df[~outlier_condition]\n",
    "\n",
    "# Create a boxplot to visualize the cleaned DataFrame without outliers\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df)\n",
    "plt.title(\"Boxplot of Cleaned Data Without Outliers\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the boxplot data graph after cleaning the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the cleaned data\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Encode the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to determine the categorical data inside the dataset first\n",
    "# However, by observing the dataset it does not have any categorical data but we can double check it by using .dtypes\n",
    "\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that all data is numerical data so we do not need to do any encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Feature Scaling\n",
    "Now we should scale down the number of column to match the objective of our project. Also scale down the numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()\n",
    "\n",
    "# Remove unwanted data columns that are irrelevant to the analysis\n",
    "drop_columns = ['Product Bounce Visitors', 'Product Bounce Rate','Likes', 'Product Visitors (Add to Cart)',\n",
    "       'Units (Add to Cart)', 'Conversion Rate (Add to Cart)','Buyers (Placed Order)', 'Units (Placed Order)', 'Items Placed',\n",
    "       'Sales (Placed Order) (MYR)', 'Conversion Rate (Placed Order)','Buyers (Confirmed Order)', 'Units (Confirmed Order)',\n",
    "       'Items Confirmed','Conversion Rate (Confirmed Order)','Conversion Rate (Placed to Confirmed)']\n",
    "\n",
    "df1.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLR CODE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Prepare the data for multiple regression model\n",
    "X =df1 [['Product Visitors (Visit)','Product Page Views','Items Visited','Search Clicks']] # Independent Variable\n",
    "y = df1['Sales (Confirmed Order) (MYR)'] #Dependent Variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "\n",
    "#Create a linear regressioin model\n",
    "obj1_model = LinearRegression()\n",
    "#train the model\n",
    "obj1_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = obj1_model.predict(X_test)\n",
    "\n",
    "#Plot the graph for predicted vs actual values\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "plt.scatter (y_test, y_pred)\n",
    "plt.xlabel ('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test, y_pred)[0], 2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see that the result of r = 0.78 indicates that this is a stong positive linear relationship between the predicted values and the actual values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph for residuals\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Plot residuals against predicted values\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.title('Predicted vs Residuals (For accessing model accuracy)')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomized order of dots below the y=0 line can define that this model is making errors in a way that is not systematically biased across all predicted values, which is a positive sign. Hence, the goal for this plot is to make sure the residuals spread randomly around the y=0 line with no discernible pattern in order to have a more accurate prediction in sales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram \n",
    "import seaborn as sns\n",
    "from scipy.stats import shapiro\n",
    "sns.distplot ((y_test - y_pred), bins = 50)\n",
    "plt.xlabel ('Residuals')\n",
    "plt.ylabel('Density')\n",
    "plt.title ('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test - y_pred)[1]))\n",
    "plt.show()\n",
    "\n",
    "# Performance Metrics\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "metrics_df1 = pd.DataFrame ({'Metric':\n",
    "['MAE',\n",
    "'MSE',\n",
    "'RMSE',\n",
    "'R-Squared'], 'Value':\n",
    "[metrics.mean_absolute_error(y_test, y_pred),\n",
    "metrics.mean_squared_error (y_test, y_pred),\n",
    "np.sqrt (metrics.mean_squared_error (y_test, y_pred)),\n",
    "metrics.explained_variance_score (y_test, y_pred)]}).round(3)\n",
    "print(metrics_df1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Buyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Buyer Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existing Buyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Existing Buyer Code"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
