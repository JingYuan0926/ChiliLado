{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChiliLado Data Analytics and Machine Learning - Group 8\n",
    "Table of Content:\n",
    "\n",
    "Step 1: Acquire the dataset\n",
    "\n",
    "Step 2: Import the libraries\n",
    "\n",
    "Step 3: Import the dataset\n",
    "\n",
    "Step 4a: Feature Selection (For customer flow and sales)\n",
    "\n",
    "Step 5a: Clean the data by dentifying and handling missing value, redundancy and outliers\n",
    "\n",
    "Step 6a: Encode the categorical data\n",
    "\n",
    "Step 7a: Feature Scaling\n",
    "\n",
    "Step 8a: Spliting dataset, training and accurancy\n",
    "\n",
    "Step 4b: Feature Selection (For new customer and repeat customer)\n",
    "\n",
    "Step 5b: Clean the data by dentifying and handling missing value, redundancy and outliers\n",
    "\n",
    "Step 6b: Encode the categorical data\n",
    "\n",
    "Step 7b: Feature Scaling\n",
    "\n",
    "Step 8b: Spliting dataset, training and accurancy\n",
    "\n",
    "Step 9: Combine both Multiple Linear Regression as one\n",
    "\n",
    "Step 10: Random Forest Model\n",
    "\n",
    "The flowchart of our work are shown in the diagram below.\n",
    "\n",
    "![FlowChart](ChiliLadoData/FlowChart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Acquire the dataset\n",
    "\n",
    "We got the data from this google drive link. https://drive.google.com/drive/folders/16BK8_d1V-A3M1WQ0neaeCwqrHPfzH7QS?usp=sharing . This link is provided from Wei Shen where he gotten it from Mr.Afiq, who is the founder of Chili Lado.\n",
    "\n",
    "We decided not to use all the datasets in this google drive, however, we only selected the datasets that are relevant to our analysis. The selected datasets are the Product Overview from the Product Folder. \n",
    "\n",
    "At first we downloaded all the dataset into our local drive by using the dowload all button.\n",
    "\n",
    "![DownloadAll](ChiliLadoData/DownloadAll.png)\n",
    "\n",
    "All the datasets is downloaded in this zipped file.\n",
    "\n",
    "![ZippedFile](ChiliLadoData/ZippedFile.png)\n",
    "![DownloadedFile](ChiliLadoData/DownloadedFile.png)\n",
    "\n",
    "Based on our observation, the zip include the Product Overview dataset which is from May 2023 to September 2023. All of them has 22 Column, however it has different number of rows. May, June, July, August, September has 32,31,32,32,31 rows respectively. The column names are:\n",
    "1. Date\n",
    "2. Product Visitors (Visit)\n",
    "3. Product Page Views\n",
    "4. Items Visited\n",
    "5. Product Bounce Visitors\n",
    "6. Product Bounce Rate\n",
    "7. Search Clicks\n",
    "8. Likes\n",
    "9. Product Visitors (Add to Cart)\n",
    "10. Units (Add to Cart)\n",
    "11. Conversion Rate (Add to Cart)\n",
    "12. Buyers (Placed Order)\n",
    "13. Units (Placed Order)\n",
    "14. Items Placed\n",
    "15. Sales (Placed Order)(MYR)\n",
    "16. Conversion Rate (Placed Order)\n",
    "17. Buyers (Confirmed Order)\n",
    "18. Units (Confirmed Order)\n",
    "19. Items Confirmed\n",
    "20. Sales (Confiremd Order)(MYR)\n",
    "21. Conversion Rate (Confirmed Order)\n",
    "22. Converison Rate (Placed to Confirmed)\n",
    "\n",
    "We first combine all the 5 files together, however we decided to do it with copy and paste instead of using python code because it only has 5 files. We use Ctrl+C to copy all the rows and use Ctrl+V to paste the copied rows into a new Excel File called MergedFile.xlsx.\n",
    "\n",
    "![CopiedFile](ChiliLadoData/CopiedFile.png)\n",
    "\n",
    "![PasteFile](ChiliLadoData/PasteFile.png)\n",
    "\n",
    "We copied all five datasets into the MergedFile. However, for May 2023, we copy the whole file including the column names, while for other months,we only copied the data. We pasted the data beneath May 2023. We followed the same process for July 2023 and subsequent months.\n",
    "\n",
    "![MayJune](ChiliLadoData/MayJune.png)\n",
    "\n",
    "To check if the data is merged correctly, we calculate the total number of rows by adding the number of days in these 5 months and the row contains attribute name which is 31 + 30 + 31 + 31 + + 30 + 1 = 154, as our MergedFile has 154 rows means that we had merged it correctly.\n",
    "\n",
    "To fulfill our objective, we require a different set of data sourced from the Dashboard of the year 2023.\n",
    "\n",
    "![Dashboard2023](ChiliLadoData/Dashboard2023.png)\n",
    "\n",
    "We get these following columns:\n",
    "\n",
    "1. Numbers of buyers\n",
    "2. Numbers of new buyers\n",
    "3. Numbers of existing buyers\n",
    "\n",
    "We replicate the procedure by copying the column and its data as the method above. Then, we paste this data into our recently created merged file.\n",
    "\n",
    "![AddedColumn](ChiliLadoData/AddedColumn.png)\n",
    "\n",
    "We found out that the figures in repeat purchase rate numbers are inaccurate. So, We perform data augmentation for two columns: the percentage of new buyers and the percentage of repeat buyers by using the data inside the dataset and Excel Function.\n",
    "\n",
    "![NewCalculation](ChiliLadoData/NewCalculation.png)\n",
    "\n",
    "![RepeatCalculation](ChiliLadoData/RepeatCalculation.png)\n",
    "\n",
    "We use If in our calculation because if the number of buyer is equal to zero, it might have division by zero error. If the number of buyers equals zero, the output is set to zero, otherwise, the division operation proceeds.  We also convert our calculation to percentage by using this function.\n",
    "\n",
    "![Percentage](ChiliLadoData/Percentage.png)\n",
    "\n",
    "We decide to use all these steps in Excel rather in Python because it is faster than code, also we want to make direct changes to our dataset rather than temporary changes only.\n",
    "\n",
    "We have 27 column and 154 rows inside the dataset. \n",
    "\n",
    "Initially, almost all the data in the Excel file was not numerical data.\n",
    "\n",
    "![ConvertData](ChiliLadoData/ConvertData.png)\n",
    "\n",
    "Therefore, we converted all the data in the dataset into numerical values by selecting the \"Convert to Number\" option in Excel to prevent potential errors. You can identify non-numeric data when the left upper corner of the cell is marked in green.\n",
    "\n",
    "![Number](ChiliLadoData/Number.png)\n",
    "\n",
    "If all the cells are white, it indicates that we have successfully converted the data into numerical values. Now, we can proceed with using Python for data preprocessing.\n",
    "\n",
    "We've also converted the date column to ensure Python recognizes it as a date, preventing unintentional calculations. This format ensures proper identification as a date type without triggering any unwanted error.\n",
    "\n",
    "![Date](ChiliLadoData/Date.png)\n",
    "\n",
    "Now the data can be used for the next few steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Import the dataset\n",
    "\n",
    "We imported the datasets from a local directory. We create a folder called ChiliLadoData and store all the datasets and images we use in that folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('MergedFile.xlsx')\n",
    "\n",
    "# Print info to show the info of the Excel file\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use print df to show the dataset\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4a: Feature Selection\n",
    "The objective of our assignment is to increase sales of chililado. However, there are many ways to increase sales. So, we opt for the most important element data in our dataset, which is to analyse customer behaviour. First is the number of customer visting chilidado (human flow) and the number of sales by using Multiple Linear Regression. Another way to increase sales is to determine the number of return customer and sales by using Single Linear Regression and Multiple Linear Regression. Finally we combine all factor together using neural network and Multiple Linear Regression.\n",
    "\n",
    "Instead of doing alltogether\n",
    "\n",
    "For Part a, the analysis will be customer flow and the number of sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame for objective 1\n",
    "df1 = df.copy()\n",
    "\n",
    "# Remove unwanted data columns that are irrelevant to the analysis\n",
    "drop_columns = ['Product Bounce Visitors', 'Product Bounce Rate','Likes', 'Product Visitors (Add to Cart)',\n",
    "       'Units (Add to Cart)', 'Conversion Rate (Add to Cart)','Buyers (Placed Order)', 'Units (Placed Order)', 'Items Placed',\n",
    "       'Sales (Placed Order) (MYR)', 'Conversion Rate (Placed Order)','Buyers (Confirmed Order)', 'Units (Confirmed Order)',\n",
    "       'Items Confirmed','Conversion Rate (Confirmed Order)','Conversion Rate (Placed to Confirmed)', 'Numbers of buyers',\n",
    "       'Numbers of new buyers', 'Numbers of existing buyers','Percentage of new buyers', 'Percentage of repeat buyers']\n",
    "\n",
    "df1.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5a: Clean the data by identifying and handling missing value, redundancy and outliers\n",
    "After careful consideration, we have opted for imputation as our preferred method for handling outliers instead of removing rows for handling outliers. This decision was made after experimenting with row removal, which resulted in the elimination of 50 rows from our already small dataset. Given the trade-off between having a more accurate but smaller dataset versus imputing outliers and retaining more data, we have prioritized preserving a larger dataset for analysis. This choice aims to strike a balance between data accuracy and quantity, acknowledging the importance of maximizing information while mitigating the impact of outliers on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the missing value of each column by using .isna(), use .sum() to sum all the missing value\n",
    "print(\"Find missing value of each column using isna()\")\n",
    "print (df1.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above output, we found out that there are no missing value in the dataset, so we do not need to delete or drop any row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine any redundancy in the dataset\n",
    "# Use .duplicate is to check if there is any duplicate data\n",
    "duplicate_rows = df1.duplicated().sum()\n",
    "duplicate_columns = df1.T.duplicated().sum()\n",
    "\n",
    "print(\"Find any duplicate values:\")\n",
    "duplicate_rows, duplicate_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above result, we found out that there are no duplicate data in this dataset, so there are no redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check for outliers in the data\n",
    "\n",
    "# Exclude the 'Date' column\n",
    "outliersdf = df1.copy()\n",
    "\n",
    "# Create a boxplot to visualize the outliers\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=outliersdf)\n",
    "plt.title(\"Boxplot of Data for Objective 1\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure is the boxplot that used to identify the outliers in the datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'outliersdf' is your original DataFrame\n",
    "\n",
    "# Calculate the first quartile (Q1), third quartile (Q3) and interquartile range (IQR)\n",
    "Q1 = outliersdf.quantile(0.25, numeric_only=True)\n",
    "Q3 = outliersdf.quantile(0.75, numeric_only=True)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Function to replace outliers with the median (or mean)\n",
    "def impute_outlier_with_median(outliersdf, q1, q3, iqr):\n",
    "    for col in outliersdf.select_dtypes(include=np.number).columns:\n",
    "        lower_bound = q1[col] - 1.5 * iqr[col]\n",
    "        upper_bound = q3[col] + 1.5 * iqr[col]\n",
    "        median_value = outliersdf[col].median()\n",
    "\n",
    "        # Replace outliers with median (you can also use mean or other metrics)\n",
    "        outliersdf[col] = np.where((outliersdf[col] < lower_bound) | (outliersdf[col] > upper_bound), median_value, outliersdf[col])\n",
    "    return outliersdf\n",
    "\n",
    "# Impute outliers in the DataFrame\n",
    "df_imputed = impute_outlier_with_median(outliersdf.copy(), Q1, Q3, IQR)\n",
    "\n",
    "df1 = df_imputed.copy()\n",
    "\n",
    "# Create a boxplot to visualize the DataFrame with imputed outliers\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df_imputed)\n",
    "plt.title(\"Boxplot of Data for Objective 1 with Imputed Outliers\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure is the boxplot of the dataset after detection and removal of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can display df1 with cleaned data\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers of row does not decrease as we choose imputation instead of removal of outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6a: Encode the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to determine the categorical data inside the dataset first\n",
    "# However, by observing the dataset it does not have any categorical data but we can double check it by using .dtypes\n",
    "print(df1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above output, we can observe that all the data is in numerical format so we do not need to do any encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7a: Feature Scaling\n",
    "Scale down the numbers in the dataset.\n",
    "\n",
    "We chose MinMaxScaler over other scaling methods due to its range from 0 to 1, providing positive values for our features. This is in contrast to feature scaling, which ranges from -1 to 1. The positive range aligns well with our preference for non-negative values, making MinMaxScaler the suitable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date column\n",
    "date_column = df1.iloc[:, 0]\n",
    "\n",
    "# Min-Max scale all columns except the date column\n",
    "minmax_data = MinMaxScaler().fit_transform(df1.iloc[:, 1:])\n",
    "\n",
    "# Combine the Min-Max scaled data with the date column\n",
    "minmax_frame = pd.DataFrame(data=minmax_data, columns=df1.columns[1:])\n",
    "minmax_frame.insert(0, df1.columns[0], date_column)\n",
    "\n",
    "# Print the first few rows\n",
    "print(minmax_frame)\n",
    "\n",
    "df1 = minmax_frame.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8a: Splitting the dataset, training and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for multiple regression model\n",
    "x_a = df1[['Product Visitors (Visit)','Product Page Views','Items Visited','Search Clicks']] # Independent Variable\n",
    "y_a = df1['Sales (Confirmed Order) (MYR)'] #Dependent Variable\n",
    "\n",
    "x_train_a, x_test_a, y_train_a, y_test_a = train_test_split(x_a,y_a,test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regressioin model\n",
    "model_a = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model_a.fit(x_train_a, y_train_a)\n",
    "\n",
    "# Predict the test set result using the trained model\n",
    "y_pred_a = model_a.predict(x_test_a)\n",
    "\n",
    "# Plot the graph for predicted vs actual values\n",
    "plt.scatter(y_test_a, y_pred_a)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test_a, y_pred_a)[0], 2))\n",
    "plt.show()\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae_a = mean_absolute_error(y_test_a, y_pred_a)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_a}\")\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_a = mean_squared_error(y_test_a, y_pred_a)\n",
    "print(f\"Mean Squared Error (MSE): {mse_a}\")\n",
    "\n",
    "# Calculate R-squared (R2)\n",
    "r2_a = r2_score(y_test_a, y_pred_a)\n",
    "print(f\"R-squared (R2): {r2_a}\")\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_a = np.sqrt(mean_squared_error(y_test_a, y_pred_a))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Multiple Linear Regression, the Pearson correlation coefficient is commonly used to assess the relationship between the predicted values and the actual values, rather than measuring the linear relationship between each independent variable and the dependent variable individually, as done in Simple Linear Regression. Also, it is normally called multiple correlation coefficient. \n",
    "\n",
    "From this we can see that the result of r = 0.64 indicates that this is a stong positive linear relationship between the predicted values and the actual values. \n",
    "\n",
    "The R-squared value of 0.3617 reveals that approximately 36.17% of the total variability in sales can be explained by the variation in the percentage of new and repeat buyers. However, the presence of other unaccounted factors contributes to the remaining 63.83% of variability.\n",
    "\n",
    "The Mean Absolute Error (MAE: 0.1646), Mean Squared Error (MSE: 0.0597), and Root Mean Squared Error (RMSE: 0.2443) are all relatively low, suggesting reasonable predictive accuracy.\n",
    "\n",
    "Insight: Sales are influenced by the number of items visited and product page views.\n",
    "\n",
    "Action: Optimize product pages with better images, detailed descriptions, and customer reviews to improve conversion rates.\n",
    "\n",
    "Strategy: Conduct A/B testing on product pages to determine which elements most positively impact sales, and adjust the website design accordingly.\n",
    "\n",
    "Insight: Product views and search clicks may provide insight into popular products.\n",
    "\n",
    "Action: Use this data to manage inventory more effectively, ensuring that high-demand products are well-stocked.\n",
    "\n",
    "Strategy: Develop a responsive supply chain strategy that can adapt to changes in product popularity as indicated by the data.\n",
    "\n",
    "Optimizing the Online Experience: Product page views and visits have a strong relationship with sales. The company should invest in optimizing the online user experience, ensuring that the website is user-friendly, mobile-responsive, and has fast loading times. This could involve A/B testing different page layouts to see which leads to more sales.\n",
    "\n",
    "Conversion Rate Optimization: Given the relationship between sales and add-to-cart rates, focusing on conversion rate optimization (CRO) could be very fruitful. Analyzing the point at which potential customers abandon their carts could provide insights into what changes could be made to increase sales. This might include streamlining the checkout process, offering free shipping, or providing immediate online assistance through chatbots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the graph for residuals\n",
    "\n",
    "# Calculate residuals\n",
    "residuals_a = y_test_a - y_pred_a\n",
    "\n",
    "# Plot residuals against predicted values\n",
    "plt.scatter(y_pred_a, residuals_a)\n",
    "plt.title('Predicted vs Residuals (For accessing model accuracy)')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The randomized order of dots above and below the y=0 line can define that this model is making errors in a way that is not systematically biased across all predicted values, which is a positive sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a density plot of the residuals, bins is number of bars\n",
    "sns.histplot((y_test_a - y_pred_a), bins = 50)\n",
    "plt.xlabel ('Residuals')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# 1 is the position of the variable, 0 is test statistic, 1 is p-value\n",
    "plt.title('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test_a - y_pred_a)[1]))\n",
    "plt.show()\n",
    "\n",
    "# P-value is 0.000, which is less than 0.05, so it is against the idea you are testing, so it is not normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4b: Feature Selection\n",
    "For Part b, the analysis will be determine number of new and return customer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame for objective 2\n",
    "df2 = df.copy()\n",
    "\n",
    "# Remove unwanted data columns that are irrelevant to the analysis\n",
    "drop_columns = ['Product Visitors (Visit)', 'Product Page Views','Items Visited', 'Product Bounce Visitors', 'Product Bounce Rate',\n",
    "       'Search Clicks', 'Likes', 'Product Visitors (Add to Cart)','Units (Add to Cart)', 'Conversion Rate (Add to Cart)',\n",
    "       'Buyers (Placed Order)', 'Units (Placed Order)', 'Items Placed','Sales (Placed Order) (MYR)', 'Conversion Rate (Placed Order)',\n",
    "       'Buyers (Confirmed Order)', 'Units (Confirmed Order)','Items Confirmed','Conversion Rate (Confirmed Order)','Conversion Rate (Placed to Confirmed)',\n",
    "       'Numbers of buyers', 'Numbers of new buyers','Numbers of existing buyers']\n",
    "\n",
    "df2.drop(columns=drop_columns, inplace=True)\n",
    "\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5b: Clean the data by dentifying and handling missing value, redundancy and outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the missing value of each column by using .isna(), use .sum() to sum all the missing value\n",
    "print(\"Find missing value of each column using isna()\")\n",
    "print (df2.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above output, we found out that there are no missing value in the dataset, so we do not need to delete or drop any row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine any redundancy in the dataset\n",
    "# Use .duplicate is to check if there is any duplicate data\n",
    "duplicate_rows = df2.duplicated().sum()\n",
    "duplicate_columns = df2.T.duplicated().sum()\n",
    "\n",
    "print(\"Find any duplicate values:\")\n",
    "duplicate_rows, duplicate_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above result, we found out that there are no duplicate data in this dataset, so there are no redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check for outliers in the data\n",
    "\n",
    "# Exclude the 'Date' column\n",
    "outliersdf2 = df2.copy()\n",
    "\n",
    "# Create a boxplot to visualize the outliers\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=outliersdf2)\n",
    "plt.title(\"Boxplot of Data for Obejctive 2\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure is the boxplot that used to identify the outliers in the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'outliersdf2' is your original DataFrame\n",
    "\n",
    "# Calculate the first quartile (Q1), third quartile (Q3) and interquartile range (IQR)\n",
    "Q1 = outliersdf2.quantile(0.25, numeric_only=True)\n",
    "Q3 = outliersdf2.quantile(0.75, numeric_only=True)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Function to replace outliers with the median (or mean)\n",
    "def impute_outlier_with_median(outliersdf2, q1, q3, iqr):\n",
    "    for col in outliersdf2.select_dtypes(include=np.number).columns:\n",
    "        lower_bound = q1[col] - 1.5 * iqr[col]\n",
    "        upper_bound = q3[col] + 1.5 * iqr[col]\n",
    "        median_value = outliersdf2[col].median()\n",
    "\n",
    "        # Replace outliers with median (you can also use mean or other metrics)\n",
    "        outliersdf2[col] = np.where((outliersdf2[col] < lower_bound) | (outliersdf2[col] > upper_bound), median_value, outliersdf2[col])\n",
    "    return outliersdf2\n",
    "\n",
    "# Impute outliers in the DataFrame\n",
    "df_imputed2 = impute_outlier_with_median(outliersdf2.copy(), Q1, Q3, IQR)\n",
    "\n",
    "df2 = df_imputed2.copy()\n",
    "\n",
    "# Create a boxplot to visualize the DataFrame with imputed outliers\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.boxplot(data=df_imputed2)\n",
    "plt.title(\"Boxplot of Data for Objective 2 with Imputed Outliers\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure is the boxplot of the dataset after detection and removal of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can display df2 with cleaned data\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6b: Encode the categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to determine the categorical data inside the dataset first\n",
    "# However, by observing the dataset it does not have any categorical data but we can double check it by using .dtypes\n",
    "print(df2.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above output, we can observe that all the data is in numerical format so we do not need to do any encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7b: Feature Scaling\n",
    "Scale down the numbers in the dataset\n",
    " We chose MinMaxScaler over other scaling methods due to its range from 0 to 1, providing positive values for our features. This is in contrast to feature scaling, which ranges from -1 to 1. The positive range aligns well with our preference for non-negative values, making MinMaxScaler the suitable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the date column\n",
    "date_column = df2.iloc[:, 0]\n",
    "\n",
    "# Min-Max scale all columns except the date column\n",
    "minmax_data = MinMaxScaler().fit_transform(df2.iloc[:, 1:])\n",
    "\n",
    "# Combine the Min-Max scaled data with the date column\n",
    "minmax_frame = pd.DataFrame(data=minmax_data, columns=df2.columns[1:])\n",
    "minmax_frame.insert(0, df2.columns[0], date_column)\n",
    "\n",
    "# Print some rows\n",
    "print(minmax_frame)\n",
    "\n",
    "df2 = minmax_frame.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8b: Splitting the dataset, training and accuracy\n",
    "\n",
    "The Simple Linear Regression model below is used to analyze the relationship between the percentage of new buyers and sales(confirmed order) (MYR). Scatter plots and regression lines are then plotted to visualize the results. Pearson's correlation coefficient is used to measure the strength and direction of a linear relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression to determine the relationship between the percentage of new buyers and the sales in confirmed order\n",
    "\n",
    "# Select independent variable (x) and dependent variable (y)\n",
    "x_b1 = df2['Percentage of new buyers']\n",
    "y_b1 = df2['Sales (Confirmed Order) (MYR)']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train_b1, x_test_b1, y_train_b1, y_test_b1 = train_test_split(x_b1.values.reshape(-1, 1), y_b1, test_size=0.20, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model_b1 = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model_b1 .fit (x_train_b1, y_train_b1)\n",
    "\n",
    "# Predict the test set result using the trained model\n",
    "y_pred_b1 = model_b1 .predict(x_test_b1)\n",
    "\n",
    "# Extract the Intercept Value, Y\n",
    "# Identify interception points\n",
    "intercept_b1 = model_b1 .intercept_\n",
    "\n",
    "# Extract the value of Coefficient, C\n",
    "# Identify coefficient values\n",
    "coefficient_b1 = model_b1 .coef_\n",
    "\n",
    "print (\"Intercept Value: \", intercept_b1)\n",
    "print (\"Coefficient: \", coefficient_b1)\n",
    "\n",
    "# Plot the actual data and regression line\n",
    "plt.scatter(x_test_b1, y_test_b1, color='black', label='Actual Data')\n",
    "plt.plot(x_test_b1, y_pred_b1, color='blue', linewidth=3, label='Regression Line')\n",
    "plt.xlabel('Percentage of new buyers')\n",
    "plt.ylabel('Sales (Confirmed Order) (MYR)')\n",
    "plt.title('Relationship between Percentage of new buyers and Sales (Confirmed Order) (MYR)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Pearson correlation coefficient (r) between variables\n",
    "correlation_coefficient, _ = pearsonr(x_b1, y_b1)\n",
    "print(f\"Pearson Correlation Coefficient (r): {correlation_coefficient}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae_b1 = mean_absolute_error(y_test_b1, y_pred_b1)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_b1}\")\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_b1 = mean_squared_error(y_test_b1, y_pred_b1)\n",
    "print(f\"Mean Squared Error (MSE): {mse_b1}\")\n",
    "\n",
    "# Calculate R-squared (R2)\n",
    "r2_b1 = r2_score(y_test_b1, y_pred_b1)\n",
    "print(f\"R-squared (R2): {r2_b1}\")\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_b1 = np.sqrt(mean_squared_error(y_test_b1, y_pred_b1))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_b1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of 0.5628 shows that there is a moderate positive linear relationship between the percentage of new buyers and sales in confirmed orders in MYR. The positive value indicates that as the number of new buyers increases, there is a tendency for sales in confirmed orders to increase, though the relationship is not particularly strong. \n",
    "\n",
    "The R-squared value of 0.3409 reveals that approximately 34.1% of the total variability in sales can be explained by the variation in the percentage of new buyers. This implies that while the presence of new buyers is a significant factor, 65.9% of the variability is influenced by other factors not considered in our model.\n",
    "\n",
    "The Mean Absolute Error (MAE: 0.1557), Mean Squared Error (MSE: 0.0617), and Root Mean Squared Error (RMSE: 0.2483) are all relatively low, suggesting reasonable predictive accuracy.\n",
    "\n",
    "The analysis shows a moderate to strong relationship between sales and the percentage of new buyers. This implies that efforts to attract new customers are yielding positive results. Therefore, targeted marketing campaigns focusing on demographics that are underrepresented in the current customer base could help to increase the number of new buyers. Additionally, personalized marketing strategies could be developed for repeat buyers to encourage brand loyalty and repeat purchases.\n",
    "\n",
    "Insight: The analysis indicated that new buyers have a moderate positive correlation with sales.\n",
    "\n",
    "Action: Develop targeted marketing campaigns aimed at attracting new customers. This could include social media advertising, collaborations with influencers, or offering first-time buyer discounts.\n",
    "\n",
    "Strategy: Use the customer flow data to identify peak times and channels that attract the most visitors, then align marketing campaigns to be most aggressive during these periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapiro-Wilk test to assess whether the residuals follow a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a density plot of the residuals, bins is number of bars\n",
    "sns.histplot((y_test_b1 - y_pred_b1), bins = 50)\n",
    "plt.xlabel ('Residuals')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# 1 is the position of the variable, 0 is test statistic, 1 is p-value\n",
    "plt.title ('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test_b1 - y_pred_b1)[1]))\n",
    "plt.show()\n",
    "\n",
    "# P-value is 0.000, which is less than 0.05, so it is against the idea you are testing, so it is not normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is 0.000, which is less than 0.05. Therefore, the conclusion is that the residuals are not normally distributed based on the results of the Shapiro-Wilk test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Simple Linear Regression model is also used to analyze the relationship between the percentage of repeat buyers and sales(confirmed order) (MYR). Scatter plots and regression lines are then plotted to visualize the results. Pearson's correlation coefficient is used to measure the strength and direction of a linear relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression to determine the relationship between the percentage ofrepeat buyers and the sales in confirmed order\n",
    "\n",
    "# Select independent variable (x) and dependent variable (y)\n",
    "x_b2 = df2['Percentage of repeat buyers']\n",
    "y_b2 = df2['Sales (Confirmed Order) (MYR)']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train_b2, x_test_b2, y_train_b2, y_test_b2 = train_test_split(x_b2.values.reshape(-1,1), y_b2, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model_b2 = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model_b2.fit (x_train_b2, y_train_b2)\n",
    "\n",
    "# Predict the test set result using the trained model\n",
    "y_pred_b2 = model_b2.predict(x_test_b2)\n",
    "\n",
    "# Extract the Intercept Value, Y\n",
    "# Identify interception points\n",
    "intercept_b2 = model_b2.intercept_\n",
    "\n",
    "# Extract the value of Coefficient, C\n",
    "# Identify coefficient values\n",
    "coefficient_b2 = model_b2.coef_\n",
    "\n",
    "print (\"Intercept Value: \", intercept_b2)\n",
    "print (\"Coefficient: \", coefficient_b2)\n",
    "\n",
    "# Plot the actual data and regression line\n",
    "plt.scatter(x_test_b2, y_test_b2, color='black', label='Actual Data')\n",
    "plt.plot(x_test_b2, y_pred_b2, color='blue', linewidth=3, label='Regression Line')\n",
    "plt.title('Relationship between Percentage of repeat buyers and Sales (Confirmed Order) (MYR)')\n",
    "plt.xlabel('Percentage of repeat buyers')\n",
    "plt.ylabel('Sales (Confirmed Order) (MYR)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Calculate Pearson correlation coefficient (r) between variables\n",
    "correlation_coefficient, _ = pearsonr(x_b2, y_b2)\n",
    "print(f\"Pearson Correlation Coefficient (r): {correlation_coefficient}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae_b2 = mean_absolute_error(y_test_b2, y_pred_b2)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_b2}\")\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_b2 = mean_squared_error(y_test_b2, y_pred_b2)\n",
    "print(f\"Mean Squared Error (MSE): {mse_b2}\")\n",
    "\n",
    "# Calculate R-squared (R2)\n",
    "r2_b2 = r2_score(y_test_b2, y_pred_b2)\n",
    "print(f\"R-squared (R2): {r2_b2}\")\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_b2 = np.sqrt(mean_squared_error(y_test_b2, y_pred_b2))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_b2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of 0.2957 shows that there is a weak positive linear relationship between the percentage of repeat buyers and sales in confirmed orders in MYR. The positive value indicates that as the number of repeat buyers increases, there is a tendency for sales in confirmed orders to increase, though the relationship is weak. \n",
    "\n",
    "The R-squared value of 0.07 reveals that approximately 7% of the total variability in sales can be explained by the variation in the percentage of repeat buyers. This implies that while the presence of repeat buyers is merly a small factor, 93% of the variability is influenced by other factors not considered in our model.\n",
    "\n",
    "The Mean Absolute Error (MAE: 0.2251), Mean Squared Error (MSE: 0.0862), and Root Mean Squared Error (RMSE: 0.2935) are all relatively low, suggesting reasonable predictive accuracy.\n",
    "\n",
    "\n",
    "With a weaker relationship between repeat buyers and sales, there's an opportunity to improve customer retention. Implementing loyalty programs, such as rewards for frequent purchases, personalized discounts, or exclusive offers for returning customers, could enhance repeat purchase rates.\n",
    "\n",
    "This insight underscores the potential and needs for improving the chili's flavour or more offer to attract customers to buy again, thereby boosting sales in confirmed orders. \n",
    "\n",
    "Insight: Repeat buyers have a weak positive correlation with sales, suggesting the need for improved retention strategies.\n",
    "\n",
    "Action: Implement a loyalty program that rewards repeat purchases with discounts, exclusive offers, or early access to new products.\n",
    "\n",
    "Strategy: Analyze the purchasing patterns of repeat buyers to tailor the loyalty program rewards to their preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a density plot of the residuals.\n",
    "\n",
    "# Create a density plot of the residuals, bins is number of bars\n",
    "sns.histplot((y_test_b2 - y_pred_b2), bins = 50)\n",
    "plt.xlabel ('Residuals')\n",
    "plt.ylabel('Density')\n",
    "# 1 is the position of the variable, 0 is test statistic, 1 is p-value\n",
    "plt.title ('Histogram of Residuals (Shapiro W p-value = {0:0.3f})'.format(shapiro(y_test_b2 - y_pred_b2)[1]))\n",
    "plt.show()\n",
    "# P-value is 0.000, which is less than 0.05, so it is against the idea you are testing, so it is not normally distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is 0.000, which is less than 0.05. Therefore, the conclusion is that the residuals are not normally distributed based on the results of the Shapiro-Wilk test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here to combine the 2 Single Linear Regression above, Multiple Linear Regression model is used to analyze the relationship between the percentage of new and repeat buyers and sales(confirmed order) (MYR). Scatter plots and regression lines are then plotted to visualize the results. Pearson's correlation coefficient is used to measure the strength and direction of a linear relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLR for Percentage of new buyers and Percentage of repeat buyers\n",
    "\n",
    "# Select independent variable (X) and dependent variable (y)\n",
    "x_bmlr = df2[['Percentage of new buyers', 'Percentage of repeat buyers']]\n",
    "y_bmlr = df2['Sales (Confirmed Order) (MYR)']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "x_train_bmlr, x_test_bmlr, y_train_bmlr, y_test_bmlr = train_test_split(x_bmlr.values.reshape(-1, 2), y_bmlr, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initiate Linear Regression\n",
    "model_bmlr = LinearRegression()\n",
    "\n",
    "# Fit model to train data\n",
    "model_bmlr.fit(x_train_bmlr, y_train_bmlr)\n",
    "\n",
    "y_pred_bmlr = model_bmlr.predict(x_test_bmlr)\n",
    "\n",
    "# Extract Intercept Value\n",
    "intercept_bmlr = model_bmlr.intercept_\n",
    "\n",
    "# Extract Coefficient Values\n",
    "coefficients_bmlr = model_bmlr.coef_\n",
    "\n",
    "print (\"Intercept Value: \", intercept_bmlr)\n",
    "print (\"Coefficient: \", coefficients_bmlr)\n",
    "\n",
    "# Visualize scatter plot for predictions vs actual values\n",
    "plt.scatter(y_test_bmlr, y_pred_bmlr, color='blue', label='Actual Data')\n",
    "\n",
    "plt.xlabel('Percentage of new and repeat buyers')\n",
    "plt.ylabel('Predicted Sales (Confirmed Order) (MYR)')\n",
    "plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test_bmlr, y_pred_bmlr)[0]))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae_bmlr = mean_absolute_error(y_test_bmlr, y_pred_bmlr)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_bmlr}\")\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_bmlr = mean_squared_error(y_test_bmlr, y_pred_bmlr)\n",
    "print(f\"Mean Squared Error (MSE): {mse_bmlr}\")\n",
    "\n",
    "# Calculate R-squared (R2)\n",
    "r2_bmlr = r2_score(y_test_bmlr, y_pred_bmlr)\n",
    "print(f\"R-squared (R2): {r2_bmlr}\")\n",
    "\n",
    "\n",
    "rmse_bmlr = np.sqrt(mean_squared_error(y_test_bmlr, y_pred_bmlr))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_bmlr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Multiple Linear Regression, the Pearson correlation coefficient is commonly used to assess the relationship between the predicted values and the actual values, rather than measuring the linear relationship between each independent variable and the dependent variable individually, as done in Simple Linear Regression. Also, it is normally called multiple correlation coefficient. \n",
    "\n",
    "The result of 0.76 shows a strong linear relationship between the overall predictions of your MLR model and the actual outcomes. This indicates that the model is performing well in explaining the variation in the data.\n",
    "\n",
    "The R-squared value of 0.3563 reveals that approximately 35.6% of the total variability in sales can be explained by the variation in the percentage of new and repeat buyers. However, the presence of other unaccounted factors contributes to the remaining 64.4% of variability.\n",
    "\n",
    "The Mean Absolute Error (MAE: 0.1487), Mean Squared Error (MSE: 0.0602), and Root Mean Squared Error (RMSE: 0.2454) are all relatively low, suggesting reasonable predictive accuracy.\n",
    "\n",
    "Here are the metric obtained from the Single Linear Regression:\n",
    "First Model\n",
    "1. Mean Absolute Error (MAE): 0.1557287631083225\n",
    "2. Mean Squared Error (MSE): 0.061653663866352265\n",
    "3. R-squared (R2): 0.34088722548671835\n",
    "4. Root Mean Squared Error (RMSE): 0.24830155832445408\n",
    "\n",
    "Second Model\n",
    "1. Mean Absolute Error (MAE): 0.22513521776646409\n",
    "2. Mean Squared Error (MSE): 0.08615952543290184\n",
    "3. R-squared (R2): 0.07890561083393577\n",
    "4. Root Mean Squared Error (RMSE): 0.29352942856364816\n",
    "\n",
    "For this Multiple Linear Regression it has the lowest MAE, MSE and RMSE while R-squared is the highest. This state that Multiple Linear Regression model is better and more suitable than Single Linear Regression model in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Combine all Multiple Linear Regression as one Multiple Linear Regression\n",
    "Here to combine the 2 Multiple Linear Regression above which is from Part A and Part B, Multiple Linear Regression model is used to analyze customer behaviour and sales(confirmed order) (MYR). Scatter plots and regression lines are then plotted to visualize the results. Pearson's correlation coefficient is used to measure the strength and direction of a linear relationship between two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate columns from both datasets\n",
    "x_9 = pd.concat([df1[['Product Visitors (Visit)', 'Product Page Views', 'Items Visited', 'Search Clicks']],\n",
    "               df2[['Percentage of new buyers', 'Percentage of repeat buyers']]], axis=1)\n",
    "\n",
    "# Target variable\n",
    "y_9 = df1['Sales (Confirmed Order) (MYR)']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train_9, x_test_9, y_train_9, y_test_9 = train_test_split(x_9, y_9, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model_9 = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model_9.fit(x_train_9, y_train_9)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_9 = model_9.predict(x_test_9)\n",
    "\n",
    "# Plotting the predicted vs actual values\n",
    "plt.scatter(y_test_9, y_pred_9)\n",
    "plt.xlabel ('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Predicted vs. Actual Values (r = {0:0.2f})'.format(pearsonr(y_test_9, y_pred_9)[0], 2))\n",
    "plt.show()\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae_9 = mean_absolute_error(y_test_9, y_pred_9)\n",
    "print(f\"Mean Absolute Error (MAE): {mae_9}\")\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_9 = mean_squared_error(y_test_9, y_pred_9)\n",
    "print(f\"Mean Squared Error (MSE): {mse_9}\")\n",
    "\n",
    "# Calculate R-squared (R2)\n",
    "r2_9 = r2_score(y_test_9, y_pred_9)\n",
    "print(f\"R-squared (R2): {r2_9}\")\n",
    "\n",
    "\n",
    "rmse_9 = np.sqrt(mean_squared_error(y_test_9, y_pred_9))\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of 0.70 shows a strong linear relationship between the overall predictions of your MLR model and the actual outcomes. This indicates that the model is performing well in explaining the variation in the data. However this value is lower before incorporating customer behavior. This means that repeat customer plays a important part in the overall sales.\n",
    "\n",
    "The R-squared value of 0.4183 reveals that approximately 41.83% of the total variability in sales can be explained by the variation in the customer behaviour. However, the presence of other unaccounted factors contributes to the remaining 58.17% of variability.\n",
    "\n",
    "The Mean Absolute Error (MAE: 0.1430), Mean Squared Error (MSE: 0.0544), and Root Mean Squared Error (RMSE: 0.2333) are all relatively low, suggesting reasonable predictive accuracy.\n",
    "\n",
    "Here is the metric obtained from the Multiple Linear Regression from Sales and Percentage of New and Repeat Customer\n",
    "\n",
    "1. Mean Absolute Error (MAE): 0.148739393014352\n",
    "2. Mean Squared Error (MSE): 0.06020542214623809\n",
    "3. R-squared (R2): 0.3563697541549179\n",
    "4. Root Mean Squared Error (RMSE): 0.24536793218804712\n",
    "\n",
    "For this Multiple Linear Regression it has a lower MAE, MSE, RMSE while it has a significant higher R2 than the Multiple Multiple Linear Regression from Sales and Percentage of New and Repeat Customer. This means that after incoporating more factor like Product Visitors, Product Page View, Items Visited and more the model becomes more accurate.\n",
    "\n",
    "This help us to indentify that the best way to increase sales is to attract customer more, as a better marketing will attract more customer and increase sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here RNN is use and the date column is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "df1['Year'] = df1['Date'].dt.year\n",
    "df1['Month'] = df1['Date'].dt.month\n",
    "df1['Day'] = df1['Date'].dt.day\n",
    "\n",
    "# Concatenate columns from both datasets, including the extracted date features\n",
    "x_9rnn = pd.concat([df1[['Year', 'Month', 'Day', 'Product Visitors (Visit)', 'Product Page Views', 'Items Visited', 'Search Clicks']],\n",
    "               df2[['Percentage of new buyers', 'Percentage of repeat buyers']]], axis=1)\n",
    "\n",
    "y_9rnn = df1['Sales (Confirmed Order) (MYR)']\n",
    "\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "x_scaled = scaler.fit_transform(x_9rnn)\n",
    "\n",
    "# Reshape data for RNN: [samples, time steps, features]\n",
    "x_rnn = np.reshape(x_scaled, (x_scaled.shape[0], 1, x_scaled.shape[1]))\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train_rnn, x_test_rnn, y_train_9rnn, y_test_9rnn = train_test_split(x_rnn, y_9rnn, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the RNN model\n",
    "model_rnn = Sequential()\n",
    "model_rnn.add(LSTM(50, return_sequences=True, input_shape=(1, x_rnn.shape[2])))\n",
    "model_rnn.add(LSTM(50))\n",
    "model_rnn.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "model_rnn.fit(x_train_rnn, y_train_9rnn, epochs=100, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rnn = model_rnn.predict(x_test_rnn)\n",
    "\n",
    "# Plotting the predicted vs actual values\n",
    "plt.scatter(y_test_9rnn, y_pred_rnn)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('RNN Predicted vs. Actual Values')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print metrics\n",
    "mae_rnn = mean_absolute_error(y_test_9rnn, y_pred_rnn)\n",
    "mse_rnn = mean_squared_error(y_test_9rnn, y_pred_rnn)\n",
    "r2_rnn = r2_score(y_test_9rnn, y_pred_rnn)\n",
    "rmse_rnn = np.sqrt(mse_rnn)\n",
    "\n",
    "print(f\"RNN Mean Absolute Error (MAE): {mae_rnn}\")\n",
    "print(f\"RNN Mean Squared Error (MSE): {mse_rnn}\")\n",
    "print(f\"RNN R-squared (R2): {r2_rnn}\")\n",
    "print(f\"RNN Root Mean Squared Error (RMSE): {rmse_rnn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Random Forest Model\n",
    "Used to capture complex non linear relationship and feature importance. This is more important\n",
    "\n",
    "Non-linear Relationships: Random Forest can capture non-linear relationships that MLR might miss. This could reveal more complex patterns in how customer behaviors interact to influence sales, allowing for more sophisticated marketing strategies.\n",
    "\n",
    "Robustness to Outliers: Random Forest is more robust to outliers than MLR, possibly providing a more accurate picture of the sales drivers if your data has outliers or is not normally distributed.\n",
    "\n",
    "Actionable Strategies: With a better understanding of what drives sales, you can create more targeted strategies. For instance, if the time on site is a key feature, you might work to create more engaging content to keep customers browsing longer.\n",
    "Risk Mitigation: Insights from the Random Forest model could also help in identifying potential risks or issues before they have a significant impact. For example, if a drop in a particular feature significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "x_10 = pd.concat([df1[['Product Visitors (Visit)', 'Product Page Views', 'Items Visited', 'Search Clicks']],\n",
    "               df2[['Percentage of new buyers', 'Percentage of repeat buyers']]], axis=1)\n",
    "\n",
    "# Target variable\n",
    "y_10 = df1['Sales (Confirmed Order) (MYR)']\n",
    "\n",
    "x_train_10, x_test_10, y_train_10, y_test_10 = train_test_split(x_10, y_10, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Regression model\n",
    "model_rf = RandomForestRegressor()\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt']\n",
    "}\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=model_rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(x_train_10, y_train_10)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Use the best parameters to create a new model\n",
    "best_rf = RandomForestRegressor(**grid_search.best_params_, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "best_rf.fit(x_train_10, y_train_10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = best_rf.predict(x_test_10)\n",
    "\n",
    "# Plotting the predicted vs actual values for Random Forest\n",
    "plt.scatter(y_test_10, y_pred_rf)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Random Forest Predicted vs. Actual Values')\n",
    "plt.show()\n",
    "\n",
    "# Calculate and print metrics for Random Forest\n",
    "mae_rf = mean_absolute_error(y_test_10, y_pred_rf)\n",
    "mse_rf = mean_squared_error(y_test_10, y_pred_rf)\n",
    "r2_rf = r2_score(y_test_10, y_pred_rf)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "\n",
    "print(f\"Random Forest Mean Absolute Error (MAE): {mae_rf}\")\n",
    "print(f\"Random Forest Mean Squared Error (MSE): {mse_rf}\")\n",
    "print(f\"Random Forest R-squared (R2): {r2_rf}\")\n",
    "print(f\"Random Forest Root Mean Squared Error (RMSE): {rmse_rf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=20,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    max_features='sqrt',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "best_rf_model.fit(x_train_10, y_train_10)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_rf = best_rf_model.predict(x_test_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = best_rf_model.feature_importances_\n",
    "features = x_10.columns  # Make sure this refers to the correct DataFrame used for training the model\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "\n",
    "# Plotting Feature Importances\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importances in Random Forest Model')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, making visitors click page is the most important feature. More resources should be allocated to this part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "Customer Behavior and Sales (Part A)\n",
    "The initial part of your analysis focused on customer behavior on the eCommerce platform. Key metrics such as page views, visits, and search clicks were linked to sales, which gives you an understanding of how engagement on the platform correlates with revenue. From these insights, you could:\n",
    "\n",
    "Optimize the User Experience: Enhance the website's user interface and user experience to increase customer engagement metrics, which are directly linked to sales. A/B testing different layouts, designs, and call-to-action placements can lead to higher page views and visits.\n",
    "\n",
    "Content Strategy: Analyze which pages or products get the most views and clicks and why. This information can guide your content strategy, allowing you to focus on popular items or improve the presentation of less-viewed products.\n",
    "\n",
    "Search Optimization: Since search clicks were included as a feature, it's important to ensure that the search functionality is optimized. Consider implementing features like auto-complete, spell correction, and personalized suggestions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New and Repeat Customers (Part B)\n",
    "Understanding the dynamics between new and repeat customers gives you leverage to tailor marketing strategies accordingly. For instance:\n",
    "\n",
    "Loyalty Programs: If repeat customers significantly contribute to sales, developing a loyalty program or subscription model could enhance customer retention rates.\n",
    "\n",
    "Targeted Marketing: Create different marketing campaigns for new and existing customers. Use insights from your data to customize these campaigns, emphasizing customer acquisition for new customers and value or loyalty for repeat customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined Model Insights\n",
    "After integrating all factors, you gain a comprehensive overview of how each element contributes to sales. Here's how you can act on these insights:\n",
    "\n",
    "Holistic Marketing Strategy: Your combined model can identify which factors have the most significant impact on sales. Allocate your marketing budget accordingly, focusing on high-impact strategies that drive both traffic and conversion rates.\n",
    "\n",
    "Product Development and Inventory Management: By understanding the full customer journey from initial visit to repeat purchase, you can make informed decisions about product development and inventory management to ensure that you're meeting customer demand.\n",
    "\n",
    "Customer Behavior and Sales: The MLR model that includes variables from both customer behavior and new/repeat customer metrics gives you an understanding of how these factors collectively influence sales. By knowing the combined effect, you can prioritize which aspects of the customer experience need improvement to boost sales.\n",
    "\n",
    "Resource Allocation: If certain features, like product views or repeat customer rates, have a stronger relationship with sales, you can allocate more resources to improve these areas. For instance, if product views are a strong predictor, you might invest in higher quality images or virtual try-on features.\n",
    "\n",
    "Marketing Strategy: The combined model can help refine marketing strategies. For example, if new customer rates are a significant predictor of sales, strategies could include targeted ads to attract new customers or promotions to convert first-time site visitors into purchasers.\n",
    "\n",
    "Customer Retention: If repeat customer rates significantly impact sales, then customer retention programs, loyalty rewards, or personalized marketing could be areas to invest in, aiming to increase repeat purchases.\n",
    "Feature Importance: Random Forest provides feature importance scores, which can help identify which factors most significantly predict sales. You might find that certain predictors have a stronger influence than initially thought, which could lead to refining the focus of business strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pmdarima import auto_arima\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it has a 'Date' column\n",
    "df = pd.read_excel('MergedFile.xlsx')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Choose the relevant column for time series analysis, e.g., 'Sales (Confirmed Order) (MYR)'\n",
    "sales_data = df['Sales (Confirmed Order) (MYR)']\n",
    "\n",
    "decomposition = seasonal_decompose(sales_data, model='additive', period=12)  # adjust 'period' based on your data's seasonality\n",
    "decomposition.plot()\n",
    "\n",
    "\n",
    "\n",
    "# Example parameters (you may need to find the best parameters through experimentation)\n",
    "order = (1, 1, 1)\n",
    "seasonal_order = (1, 1, 1, 12)  # Assuming annual seasonality; change the last number based on your seasonality period\n",
    "\n",
    "# Fit the SARIMA model\n",
    "sarima_model = SARIMAX(sales_data, order=order, seasonal_order=seasonal_order, enforce_stationarity=False, enforce_invertibility=False)\n",
    "sarima_results = sarima_model.fit()\n",
    "\n",
    "# Predictions\n",
    "sarima_pred = sarima_results.get_forecast(steps=12)  # Adjust steps as needed\n",
    "pred_conf = sarima_pred.conf_int()\n",
    "\n",
    "# Evaluate performance (you may need to compare these predictions with actual values)\n",
    "# ... (use metrics like Mean Squared Error, etc.)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sales_data.index, sales_data, label='Actual')\n",
    "plt.plot(sarima_pred.predicted_mean.index, sarima_pred.predicted_mean, label='Forecast')\n",
    "plt.fill_between(pred_conf.index, pred_conf.iloc[:, 0], pred_conf.iloc[:, 1], color='k', alpha=0.2)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Sales Forecast')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "auto_model = auto_arima(sales_data, seasonal=True, m=12, stepwise=True, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "auto_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_model = SARIMAX(sales_data,\n",
    "                       order=(1, 1, 1),\n",
    "                       seasonal_order=(0, 0, 1, 12),\n",
    "                       enforce_stationarity=False,\n",
    "                       enforce_invertibility=False)\n",
    "\n",
    "# Fit the model\n",
    "sarima_results = sarima_model.fit()\n",
    "\n",
    "# Summarize the model results\n",
    "print(sarima_results.summary())\n",
    "\n",
    "# Forecasting\n",
    "sarima_pred = sarima_results.get_forecast(steps=12)  # Adjust the steps as needed\n",
    "pred_conf = sarima_pred.conf_int()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sales_data.index, sales_data, label='Actual')\n",
    "plt.plot(sarima_pred.predicted_mean.index, sarima_pred.predicted_mean, label='Forecast')\n",
    "plt.fill_between(pred_conf.index, pred_conf.iloc[:, 0], pred_conf.iloc[:, 1], color='k', alpha=0.2)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Sales Forecast')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
